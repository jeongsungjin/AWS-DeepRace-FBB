1.5h

Gradient descent batch size 128
Entropy 0.01
Discount factor 0.99
Loss type Huber
Learning rate 0.0008
Number of experience episodes between each policy-updating iteration 20
Number of epochs 10

1
00:41.556
--
2
00:36.802
--
3
00:39.331

1:57