Gradient descent batch size 64
Entropy 0.03
Discount factor 0.99
Loss type Huber
Learning rate 0.001
Number of experience episodes between each policy-updating iteration 20
Number of epochs 10

1
00:49.539
--
2
00:51.275
--
3
01:01.469

2:42